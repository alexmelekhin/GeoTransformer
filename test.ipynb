{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from easydict import EasyDict as edict\n",
    "import open3d as o3d\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "\n",
    "from geotransformer.utils.registration import compute_registration_error\n",
    "from geotransformer.utils.torch import to_cuda, release_cuda\n",
    "from geotransformer.utils.open3d import make_open3d_point_cloud, get_color, draw_geometries\n",
    "from geotransformer.utils.data import registration_collate_fn_stack_mode, calibrate_neighbors_stack_mode\n",
    "from geotransformer.utils.common import ensure_dir\n",
    "from geotransformer.modules.kpconv import ConvBlock, ResidualBlock, UnaryBlock, LastUnaryBlock, nearest_upsample\n",
    "from geotransformer.modules.ops import point_to_node_partition, index_select\n",
    "from geotransformer.modules.registration import get_node_correspondences\n",
    "from geotransformer.modules.sinkhorn import LearnableLogOptimalTransport\n",
    "from geotransformer.modules.geotransformer import (\n",
    "    GeometricTransformer,\n",
    "    SuperPointMatching,\n",
    "    SuperPointTargetGenerator,\n",
    "    LocalGlobalRegistration,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_C = edict()\n",
    "\n",
    "# random seed\n",
    "_C.seed = 7351\n",
    "\n",
    "# dirs\n",
    "# _C.working_dir = osp.dirname(osp.realpath(__file__))\n",
    "# _C.root_dir = osp.dirname(osp.dirname(_C.working_dir))\n",
    "# _C.exp_name = osp.basename(_C.working_dir)\n",
    "# _C.output_dir = osp.join(_C.root_dir, 'output', _C.exp_name)\n",
    "# _C.snapshot_dir = osp.join(_C.output_dir, 'snapshots')\n",
    "# _C.log_dir = osp.join(_C.output_dir, 'logs')\n",
    "# _C.event_dir = osp.join(_C.output_dir, 'events')\n",
    "# _C.feature_dir = osp.join(_C.output_dir, 'features')\n",
    "\n",
    "# ensure_dir(_C.output_dir)\n",
    "# ensure_dir(_C.snapshot_dir)\n",
    "# ensure_dir(_C.log_dir)\n",
    "# ensure_dir(_C.event_dir)\n",
    "# ensure_dir(_C.feature_dir)\n",
    "\n",
    "# data\n",
    "_C.data = edict()\n",
    "# _C.data.dataset_root = osp.join(_C.root_dir, 'data', 'Kitti')\n",
    "\n",
    "# train data\n",
    "_C.train = edict()\n",
    "_C.train.batch_size = 1\n",
    "_C.train.num_workers = 8\n",
    "_C.train.point_limit = 30000\n",
    "_C.train.use_augmentation = True\n",
    "_C.train.augmentation_noise = 0.01\n",
    "_C.train.augmentation_min_scale = 0.8\n",
    "_C.train.augmentation_max_scale = 1.2\n",
    "_C.train.augmentation_shift = 2.0\n",
    "_C.train.augmentation_rotation = 1.0\n",
    "\n",
    "# test config\n",
    "_C.test = edict()\n",
    "_C.test.batch_size = 1\n",
    "_C.test.num_workers = 8\n",
    "_C.test.point_limit = None\n",
    "\n",
    "# eval config\n",
    "_C.eval = edict()\n",
    "_C.eval.acceptance_overlap = 0.0\n",
    "_C.eval.acceptance_radius = 1.0\n",
    "_C.eval.inlier_ratio_threshold = 0.05\n",
    "_C.eval.rre_threshold = 5.0\n",
    "_C.eval.rte_threshold = 2.0\n",
    "\n",
    "# ransac\n",
    "_C.ransac = edict()\n",
    "_C.ransac.distance_threshold = 0.3\n",
    "_C.ransac.num_points = 4\n",
    "_C.ransac.num_iterations = 50000\n",
    "\n",
    "# optim config\n",
    "_C.optim = edict()\n",
    "_C.optim.lr = 1e-4\n",
    "_C.optim.lr_decay = 0.95\n",
    "_C.optim.lr_decay_steps = 4\n",
    "_C.optim.weight_decay = 1e-6\n",
    "_C.optim.max_epoch = 160\n",
    "_C.optim.grad_acc_steps = 1\n",
    "\n",
    "# model - backbone\n",
    "_C.backbone = edict()\n",
    "_C.backbone.num_stages = 5\n",
    "_C.backbone.init_voxel_size = 0.3\n",
    "_C.backbone.kernel_size = 15\n",
    "_C.backbone.base_radius = 4.25\n",
    "_C.backbone.base_sigma = 2.0\n",
    "_C.backbone.init_radius = _C.backbone.base_radius * _C.backbone.init_voxel_size\n",
    "_C.backbone.init_sigma = _C.backbone.base_sigma * _C.backbone.init_voxel_size\n",
    "_C.backbone.group_norm = 32\n",
    "_C.backbone.input_dim = 1\n",
    "_C.backbone.init_dim = 64\n",
    "_C.backbone.output_dim = 256\n",
    "\n",
    "# model - Global\n",
    "_C.model = edict()\n",
    "_C.model.ground_truth_matching_radius = 0.6\n",
    "_C.model.num_points_in_patch = 128\n",
    "_C.model.num_sinkhorn_iterations = 100\n",
    "\n",
    "# model - Coarse Matching\n",
    "_C.coarse_matching = edict()\n",
    "_C.coarse_matching.num_targets = 128\n",
    "_C.coarse_matching.overlap_threshold = 0.1\n",
    "_C.coarse_matching.num_correspondences = 256\n",
    "_C.coarse_matching.dual_normalization = True\n",
    "\n",
    "# model - GeoTransformer\n",
    "_C.geotransformer = edict()\n",
    "_C.geotransformer.input_dim = 2048\n",
    "_C.geotransformer.hidden_dim = 128\n",
    "_C.geotransformer.output_dim = 256\n",
    "_C.geotransformer.num_heads = 4\n",
    "_C.geotransformer.blocks = ['self', 'cross', 'self', 'cross', 'self', 'cross']\n",
    "_C.geotransformer.sigma_d = 4.8\n",
    "_C.geotransformer.sigma_a = 15\n",
    "_C.geotransformer.angle_k = 3\n",
    "_C.geotransformer.reduction_a = 'max'\n",
    "\n",
    "# model - Fine Matching\n",
    "_C.fine_matching = edict()\n",
    "_C.fine_matching.topk = 2\n",
    "_C.fine_matching.acceptance_radius = 0.6\n",
    "_C.fine_matching.mutual = True\n",
    "_C.fine_matching.confidence_threshold = 0.05\n",
    "_C.fine_matching.use_dustbin = False\n",
    "_C.fine_matching.use_global_score = False\n",
    "_C.fine_matching.correspondence_threshold = 3\n",
    "_C.fine_matching.correspondence_limit = None\n",
    "_C.fine_matching.num_refinement_steps = 5\n",
    "\n",
    "# loss - Coarse level\n",
    "_C.coarse_loss = edict()\n",
    "_C.coarse_loss.positive_margin = 0.1\n",
    "_C.coarse_loss.negative_margin = 1.4\n",
    "_C.coarse_loss.positive_optimal = 0.1\n",
    "_C.coarse_loss.negative_optimal = 1.4\n",
    "_C.coarse_loss.log_scale = 40\n",
    "_C.coarse_loss.positive_overlap = 0.1\n",
    "\n",
    "# loss - Fine level\n",
    "_C.fine_loss = edict()\n",
    "_C.fine_loss.positive_radius = 0.6\n",
    "\n",
    "# loss - Overall\n",
    "_C.loss = edict()\n",
    "_C.loss.weight_coarse_loss = 1.0\n",
    "_C.loss.weight_fine_loss = 1.0\n",
    "\n",
    "\n",
    "def make_cfg():\n",
    "    return _C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPConvFPN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, init_dim, kernel_size, init_radius, init_sigma, group_norm):\n",
    "        super(KPConvFPN, self).__init__()\n",
    "\n",
    "        self.encoder1_1 = ConvBlock(input_dim, init_dim, kernel_size, init_radius, init_sigma, group_norm)\n",
    "        self.encoder1_2 = ResidualBlock(init_dim, init_dim * 2, kernel_size, init_radius, init_sigma, group_norm)\n",
    "\n",
    "        self.encoder2_1 = ResidualBlock(\n",
    "            init_dim * 2, init_dim * 2, kernel_size, init_radius, init_sigma, group_norm, strided=True\n",
    "        )\n",
    "        self.encoder2_2 = ResidualBlock(\n",
    "            init_dim * 2, init_dim * 4, kernel_size, init_radius * 2, init_sigma * 2, group_norm\n",
    "        )\n",
    "        self.encoder2_3 = ResidualBlock(\n",
    "            init_dim * 4, init_dim * 4, kernel_size, init_radius * 2, init_sigma * 2, group_norm\n",
    "        )\n",
    "\n",
    "        self.encoder3_1 = ResidualBlock(\n",
    "            init_dim * 4,\n",
    "            init_dim * 4,\n",
    "            kernel_size,\n",
    "            init_radius * 2,\n",
    "            init_sigma * 2,\n",
    "            group_norm,\n",
    "            strided=True,\n",
    "        )\n",
    "        self.encoder3_2 = ResidualBlock(\n",
    "            init_dim * 4, init_dim * 8, kernel_size, init_radius * 4, init_sigma * 4, group_norm\n",
    "        )\n",
    "        self.encoder3_3 = ResidualBlock(\n",
    "            init_dim * 8, init_dim * 8, kernel_size, init_radius * 4, init_sigma * 4, group_norm\n",
    "        )\n",
    "\n",
    "        self.encoder4_1 = ResidualBlock(\n",
    "            init_dim * 8,\n",
    "            init_dim * 8,\n",
    "            kernel_size,\n",
    "            init_radius * 4,\n",
    "            init_sigma * 4,\n",
    "            group_norm,\n",
    "            strided=True,\n",
    "        )\n",
    "        self.encoder4_2 = ResidualBlock(\n",
    "            init_dim * 8, init_dim * 16, kernel_size, init_radius * 8, init_sigma * 8, group_norm\n",
    "        )\n",
    "        self.encoder4_3 = ResidualBlock(\n",
    "            init_dim * 16, init_dim * 16, kernel_size, init_radius * 8, init_sigma * 8, group_norm\n",
    "        )\n",
    "\n",
    "        self.encoder5_1 = ResidualBlock(\n",
    "            init_dim * 16,\n",
    "            init_dim * 16,\n",
    "            kernel_size,\n",
    "            init_radius * 8,\n",
    "            init_sigma * 8,\n",
    "            group_norm,\n",
    "            strided=True,\n",
    "        )\n",
    "        self.encoder5_2 = ResidualBlock(\n",
    "            init_dim * 16, init_dim * 32, kernel_size, init_radius * 16, init_sigma * 16, group_norm\n",
    "        )\n",
    "        self.encoder5_3 = ResidualBlock(\n",
    "            init_dim * 32, init_dim * 32, kernel_size, init_radius * 16, init_sigma * 16, group_norm\n",
    "        )\n",
    "\n",
    "        self.decoder4 = UnaryBlock(init_dim * 48, init_dim * 16, group_norm)\n",
    "        self.decoder3 = UnaryBlock(init_dim * 24, init_dim * 8, group_norm)\n",
    "        self.decoder2 = LastUnaryBlock(init_dim * 12, output_dim)\n",
    "\n",
    "    def forward(self, feats, data_dict):\n",
    "        feats_list = []\n",
    "\n",
    "        points_list = data_dict['points']\n",
    "        neighbors_list = data_dict['neighbors']\n",
    "        subsampling_list = data_dict['subsampling']\n",
    "        upsampling_list = data_dict['upsampling']\n",
    "\n",
    "        feats_s1 = feats\n",
    "        feats_s1 = self.encoder1_1(feats_s1, points_list[0], points_list[0], neighbors_list[0])\n",
    "        feats_s1 = self.encoder1_2(feats_s1, points_list[0], points_list[0], neighbors_list[0])\n",
    "\n",
    "        feats_s2 = self.encoder2_1(feats_s1, points_list[1], points_list[0], subsampling_list[0])\n",
    "        feats_s2 = self.encoder2_2(feats_s2, points_list[1], points_list[1], neighbors_list[1])\n",
    "        feats_s2 = self.encoder2_3(feats_s2, points_list[1], points_list[1], neighbors_list[1])\n",
    "\n",
    "        feats_s3 = self.encoder3_1(feats_s2, points_list[2], points_list[1], subsampling_list[1])\n",
    "        feats_s3 = self.encoder3_2(feats_s3, points_list[2], points_list[2], neighbors_list[2])\n",
    "        feats_s3 = self.encoder3_3(feats_s3, points_list[2], points_list[2], neighbors_list[2])\n",
    "\n",
    "        feats_s4 = self.encoder4_1(feats_s3, points_list[3], points_list[2], subsampling_list[2])\n",
    "        feats_s4 = self.encoder4_2(feats_s4, points_list[3], points_list[3], neighbors_list[3])\n",
    "        feats_s4 = self.encoder4_3(feats_s4, points_list[3], points_list[3], neighbors_list[3])\n",
    "\n",
    "        feats_s5 = self.encoder5_1(feats_s4, points_list[4], points_list[3], subsampling_list[3])\n",
    "        feats_s5 = self.encoder5_2(feats_s5, points_list[4], points_list[4], neighbors_list[4])\n",
    "        feats_s5 = self.encoder5_3(feats_s5, points_list[4], points_list[4], neighbors_list[4])\n",
    "\n",
    "        latent_s5 = feats_s5\n",
    "        feats_list.append(feats_s5)\n",
    "\n",
    "        latent_s4 = nearest_upsample(latent_s5, upsampling_list[3])\n",
    "        latent_s4 = torch.cat([latent_s4, feats_s4], dim=1)\n",
    "        latent_s4 = self.decoder4(latent_s4)\n",
    "        feats_list.append(latent_s4)\n",
    "\n",
    "        latent_s3 = nearest_upsample(latent_s4, upsampling_list[2])\n",
    "        latent_s3 = torch.cat([latent_s3, feats_s3], dim=1)\n",
    "        latent_s3 = self.decoder3(latent_s3)\n",
    "        feats_list.append(latent_s3)\n",
    "\n",
    "        latent_s2 = nearest_upsample(latent_s3, upsampling_list[1])\n",
    "        latent_s2 = torch.cat([latent_s2, feats_s2], dim=1)\n",
    "        latent_s2 = self.decoder2(latent_s2)\n",
    "        feats_list.append(latent_s2)\n",
    "\n",
    "        feats_list.reverse()\n",
    "\n",
    "        return feats_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(GeoTransformer, self).__init__()\n",
    "        self.num_points_in_patch = cfg.model.num_points_in_patch\n",
    "        self.matching_radius = cfg.model.ground_truth_matching_radius\n",
    "\n",
    "        self.backbone = KPConvFPN(\n",
    "            cfg.backbone.input_dim,\n",
    "            cfg.backbone.output_dim,\n",
    "            cfg.backbone.init_dim,\n",
    "            cfg.backbone.kernel_size,\n",
    "            cfg.backbone.init_radius,\n",
    "            cfg.backbone.init_sigma,\n",
    "            cfg.backbone.group_norm,\n",
    "        )\n",
    "\n",
    "        self.transformer = GeometricTransformer(\n",
    "            cfg.geotransformer.input_dim,\n",
    "            cfg.geotransformer.output_dim,\n",
    "            cfg.geotransformer.hidden_dim,\n",
    "            cfg.geotransformer.num_heads,\n",
    "            cfg.geotransformer.blocks,\n",
    "            cfg.geotransformer.sigma_d,\n",
    "            cfg.geotransformer.sigma_a,\n",
    "            cfg.geotransformer.angle_k,\n",
    "            reduction_a=cfg.geotransformer.reduction_a,\n",
    "        )\n",
    "\n",
    "        self.coarse_target = SuperPointTargetGenerator(\n",
    "            cfg.coarse_matching.num_targets, cfg.coarse_matching.overlap_threshold\n",
    "        )\n",
    "\n",
    "        self.coarse_matching = SuperPointMatching(\n",
    "            cfg.coarse_matching.num_correspondences, cfg.coarse_matching.dual_normalization\n",
    "        )\n",
    "\n",
    "        self.fine_matching = LocalGlobalRegistration(\n",
    "            cfg.fine_matching.topk,\n",
    "            cfg.fine_matching.acceptance_radius,\n",
    "            mutual=cfg.fine_matching.mutual,\n",
    "            confidence_threshold=cfg.fine_matching.confidence_threshold,\n",
    "            use_dustbin=cfg.fine_matching.use_dustbin,\n",
    "            use_global_score=cfg.fine_matching.use_global_score,\n",
    "            correspondence_threshold=cfg.fine_matching.correspondence_threshold,\n",
    "            correspondence_limit=cfg.fine_matching.correspondence_limit,\n",
    "            num_refinement_steps=cfg.fine_matching.num_refinement_steps,\n",
    "        )\n",
    "\n",
    "        self.optimal_transport = LearnableLogOptimalTransport(cfg.model.num_sinkhorn_iterations)\n",
    "\n",
    "    def forward(self, data_dict):\n",
    "        output_dict = {}\n",
    "\n",
    "        # Downsample point clouds\n",
    "        feats = data_dict['features'].detach()\n",
    "        transform = data_dict['transform'].detach()\n",
    "\n",
    "        ref_length_c = data_dict['lengths'][-1][0].item()\n",
    "        ref_length_f = data_dict['lengths'][1][0].item()\n",
    "        ref_length = data_dict['lengths'][0][0].item()\n",
    "        points_c = data_dict['points'][-1].detach()\n",
    "        points_f = data_dict['points'][1].detach()\n",
    "        points = data_dict['points'][0].detach()\n",
    "\n",
    "        ref_points_c = points_c[:ref_length_c]\n",
    "        src_points_c = points_c[ref_length_c:]\n",
    "        ref_points_f = points_f[:ref_length_f]\n",
    "        src_points_f = points_f[ref_length_f:]\n",
    "        ref_points = points[:ref_length]\n",
    "        src_points = points[ref_length:]\n",
    "\n",
    "        output_dict['ref_points_c'] = ref_points_c\n",
    "        output_dict['src_points_c'] = src_points_c\n",
    "        output_dict['ref_points_f'] = ref_points_f\n",
    "        output_dict['src_points_f'] = src_points_f\n",
    "        output_dict['ref_points'] = ref_points\n",
    "        output_dict['src_points'] = src_points\n",
    "\n",
    "        # 1. Generate ground truth node correspondences\n",
    "        _, ref_node_masks, ref_node_knn_indices, ref_node_knn_masks = point_to_node_partition(\n",
    "            ref_points_f, ref_points_c, self.num_points_in_patch\n",
    "        )\n",
    "        _, src_node_masks, src_node_knn_indices, src_node_knn_masks = point_to_node_partition(\n",
    "            src_points_f, src_points_c, self.num_points_in_patch\n",
    "        )\n",
    "\n",
    "        ref_padded_points_f = torch.cat([ref_points_f, torch.zeros_like(ref_points_f[:1])], dim=0)\n",
    "        src_padded_points_f = torch.cat([src_points_f, torch.zeros_like(src_points_f[:1])], dim=0)\n",
    "        ref_node_knn_points = index_select(ref_padded_points_f, ref_node_knn_indices, dim=0)\n",
    "        src_node_knn_points = index_select(src_padded_points_f, src_node_knn_indices, dim=0)\n",
    "\n",
    "        gt_node_corr_indices, gt_node_corr_overlaps = get_node_correspondences(\n",
    "            ref_points_c,\n",
    "            src_points_c,\n",
    "            ref_node_knn_points,\n",
    "            src_node_knn_points,\n",
    "            transform,\n",
    "            self.matching_radius,\n",
    "            ref_masks=ref_node_masks,\n",
    "            src_masks=src_node_masks,\n",
    "            ref_knn_masks=ref_node_knn_masks,\n",
    "            src_knn_masks=src_node_knn_masks,\n",
    "        )\n",
    "\n",
    "        output_dict['gt_node_corr_indices'] = gt_node_corr_indices\n",
    "        output_dict['gt_node_corr_overlaps'] = gt_node_corr_overlaps\n",
    "\n",
    "        # 2. KPFCNN Encoder\n",
    "        feats_list = self.backbone(feats, data_dict)\n",
    "\n",
    "        feats_c = feats_list[-1]\n",
    "        feats_f = feats_list[0]\n",
    "\n",
    "        # 3. Conditional Transformer\n",
    "        ref_feats_c = feats_c[:ref_length_c]\n",
    "        src_feats_c = feats_c[ref_length_c:]\n",
    "        ref_feats_c, src_feats_c = self.transformer(\n",
    "            ref_points_c.unsqueeze(0),\n",
    "            src_points_c.unsqueeze(0),\n",
    "            ref_feats_c.unsqueeze(0),\n",
    "            src_feats_c.unsqueeze(0),\n",
    "        )\n",
    "        ref_feats_c_norm = F.normalize(ref_feats_c.squeeze(0), p=2, dim=1)\n",
    "        src_feats_c_norm = F.normalize(src_feats_c.squeeze(0), p=2, dim=1)\n",
    "\n",
    "        output_dict['ref_feats_c'] = ref_feats_c_norm\n",
    "        output_dict['src_feats_c'] = src_feats_c_norm\n",
    "\n",
    "        # 5. Head for fine level matching\n",
    "        ref_feats_f = feats_f[:ref_length_f]\n",
    "        src_feats_f = feats_f[ref_length_f:]\n",
    "        output_dict['ref_feats_f'] = ref_feats_f\n",
    "        output_dict['src_feats_f'] = src_feats_f\n",
    "\n",
    "        # 6. Select topk nearest node correspondences\n",
    "        with torch.no_grad():\n",
    "            ref_node_corr_indices, src_node_corr_indices, node_corr_scores = self.coarse_matching(\n",
    "                ref_feats_c_norm, src_feats_c_norm, ref_node_masks, src_node_masks\n",
    "            )\n",
    "\n",
    "            output_dict['ref_node_corr_indices'] = ref_node_corr_indices\n",
    "            output_dict['src_node_corr_indices'] = src_node_corr_indices\n",
    "\n",
    "            # 7 Random select ground truth node correspondences during training\n",
    "            if self.training:\n",
    "                ref_node_corr_indices, src_node_corr_indices, node_corr_scores = self.coarse_target(\n",
    "                    gt_node_corr_indices, gt_node_corr_overlaps\n",
    "                )\n",
    "\n",
    "        # 7.2 Generate batched node points & feats\n",
    "        ref_node_corr_knn_indices = ref_node_knn_indices[ref_node_corr_indices]  # (P, K)\n",
    "        src_node_corr_knn_indices = src_node_knn_indices[src_node_corr_indices]  # (P, K)\n",
    "        ref_node_corr_knn_masks = ref_node_knn_masks[ref_node_corr_indices]  # (P, K)\n",
    "        src_node_corr_knn_masks = src_node_knn_masks[src_node_corr_indices]  # (P, K)\n",
    "        ref_node_corr_knn_points = ref_node_knn_points[ref_node_corr_indices]  # (P, K, 3)\n",
    "        src_node_corr_knn_points = src_node_knn_points[src_node_corr_indices]  # (P, K, 3)\n",
    "\n",
    "        ref_padded_feats_f = torch.cat([ref_feats_f, torch.zeros_like(ref_feats_f[:1])], dim=0)\n",
    "        src_padded_feats_f = torch.cat([src_feats_f, torch.zeros_like(src_feats_f[:1])], dim=0)\n",
    "        ref_node_corr_knn_feats = index_select(ref_padded_feats_f, ref_node_corr_knn_indices, dim=0)  # (P, K, C)\n",
    "        src_node_corr_knn_feats = index_select(src_padded_feats_f, src_node_corr_knn_indices, dim=0)  # (P, K, C)\n",
    "\n",
    "        output_dict['ref_node_corr_knn_points'] = ref_node_corr_knn_points\n",
    "        output_dict['src_node_corr_knn_points'] = src_node_corr_knn_points\n",
    "        output_dict['ref_node_corr_knn_masks'] = ref_node_corr_knn_masks\n",
    "        output_dict['src_node_corr_knn_masks'] = src_node_corr_knn_masks\n",
    "\n",
    "        # 8. Optimal transport\n",
    "        matching_scores = torch.einsum('bnd,bmd->bnm', ref_node_corr_knn_feats, src_node_corr_knn_feats)  # (P, K, K)\n",
    "        matching_scores = matching_scores / feats_f.shape[1] ** 0.5\n",
    "        matching_scores = self.optimal_transport(matching_scores, ref_node_corr_knn_masks, src_node_corr_knn_masks)\n",
    "\n",
    "        output_dict['matching_scores'] = matching_scores\n",
    "\n",
    "        # 9. Generate final correspondences during testing\n",
    "        with torch.no_grad():\n",
    "            if not self.fine_matching.use_dustbin:\n",
    "                matching_scores = matching_scores[:, :-1, :-1]\n",
    "\n",
    "            ref_corr_points, src_corr_points, corr_scores, estimated_transform = self.fine_matching(\n",
    "                ref_node_corr_knn_points,\n",
    "                src_node_corr_knn_points,\n",
    "                ref_node_corr_knn_masks,\n",
    "                src_node_corr_knn_masks,\n",
    "                matching_scores,\n",
    "                node_corr_scores,\n",
    "            )\n",
    "\n",
    "            output_dict['ref_corr_points'] = ref_corr_points\n",
    "            output_dict['src_corr_points'] = src_corr_points\n",
    "            output_dict['corr_scores'] = corr_scores\n",
    "            output_dict['estimated_transform'] = estimated_transform\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "def load_pc(filepath, max_point_distance = 100, voxel_size = 0.3) -> np.ndarray:\n",
    "    pc = np.fromfile(filepath, dtype=np.float32).reshape((-1, 4))[:, :-1]\n",
    "    in_range_idx = np.all(\n",
    "        np.logical_and(-100 <= pc, pc <= 100),  # select points in range [-100, 100] meters\n",
    "        axis=1,\n",
    "    )\n",
    "    pc = pc[in_range_idx]\n",
    "    if max_point_distance is not None:\n",
    "        pc = pc[np.linalg.norm(pc, axis=1) < max_point_distance]\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pc)\n",
    "    pcd = pcd.voxel_down_sample(voxel_size)\n",
    "    pc = np.array(pcd.points).astype(np.float32)\n",
    "    return pc\n",
    "\n",
    "def get_transform_from_rotation_translation(rotation: np.ndarray, translation: np.ndarray) -> np.ndarray:\n",
    "    r\"\"\"Get rigid transform matrix from rotation matrix and translation vector.\n",
    "\n",
    "    Args:\n",
    "        rotation (array): (3, 3)\n",
    "        translation (array): (3,)\n",
    "\n",
    "    Returns:\n",
    "        transform: (4, 4)\n",
    "    \"\"\"\n",
    "    transform = np.eye(4)\n",
    "    transform[:3, :3] = rotation\n",
    "    transform[:3, 3] = translation\n",
    "    return transform\n",
    "\n",
    "\n",
    "def calculate_transform(ref, src):\n",
    "    ref_t = ref[:3]\n",
    "    src_t = src[:3]\n",
    "    ref_q = ref[3:]\n",
    "    src_q = src[3:]\n",
    "    ref_rot = R.from_quat(ref_q).as_matrix()\n",
    "    src_rot = R.from_quat(src_q).as_matrix()\n",
    "    ref_transform = get_transform_from_rotation_translation(ref_rot, ref_t)\n",
    "    src_transform = get_transform_from_rotation_translation(src_rot, src_t)\n",
    "    src_to_ref_transform = np.dot(np.linalg.inv(ref_transform), src_transform)\n",
    "    return src_to_ref_transform.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = make_cfg()\n",
    "\n",
    "model = GeoTransformer(config)\n",
    "\n",
    "weights_path =\"./weights/geotransformer-kitti.pth.tar\"\n",
    "weights = torch.load(weights_path, map_location=torch.device('cpu'))[\"model\"]\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/docker_geotransformer/Datasets/ITLP-Campus-data/subsampled_data/outdoor/03_2023-04-11-day\"\n",
    "\n",
    "track_df =pd.read_csv(data_dir + \"/track.csv\")\n",
    "\n",
    "rre_list = []\n",
    "rte_list = []\n",
    "\n",
    "for i in range(2, len(track_df)):\n",
    "    ref_points = load_pc(data_dir + f\"/lidar/{track_df['lidar_ts'].iloc[i-1]}.bin\")\n",
    "    src_points = load_pc(data_dir + f\"/lidar/{track_df['lidar_ts'].iloc[i]}.bin\")\n",
    "\n",
    "    ref_feats = np.ones((ref_points.shape[0], 1), dtype=np.float32)\n",
    "    src_feats = np.ones((src_points.shape[0], 1), dtype=np.float32)\n",
    "    gt_transform = calculate_transform(\n",
    "        track_df[[\"tx\", \"ty\", \"tz\", \"qx\", \"qy\", \"qz\", \"qw\"]].iloc[i-1].to_numpy().astype(np.float32),\n",
    "        track_df[[\"tx\", \"ty\", \"tz\", \"qx\", \"qy\", \"qz\", \"qw\"]].iloc[i].to_numpy().astype(np.float32),\n",
    "    )\n",
    "\n",
    "    data_dict = {}\n",
    "    data_dict['ref_points'] = ref_points\n",
    "    data_dict['src_points'] = src_points\n",
    "    data_dict['ref_feats'] = ref_feats\n",
    "    data_dict['src_feats'] = src_feats\n",
    "    data_dict['transform'] = gt_transform\n",
    "\n",
    "    neighbor_limits = calibrate_neighbors_stack_mode(\n",
    "        [data_dict],\n",
    "        registration_collate_fn_stack_mode,\n",
    "        config.backbone.num_stages,\n",
    "        config.backbone.init_voxel_size,\n",
    "        config.backbone.init_radius,\n",
    "    )\n",
    "\n",
    "    data_dict = registration_collate_fn_stack_mode(\n",
    "        [data_dict], config.backbone.num_stages, config.backbone.init_voxel_size, config.backbone.init_radius, neighbor_limits\n",
    "    )\n",
    "    data_dict = to_cuda(data_dict)\n",
    "\n",
    "    output_dict = model(data_dict)\n",
    "\n",
    "    data_dict = release_cuda(data_dict)\n",
    "    output_dict = release_cuda(output_dict)\n",
    "\n",
    "    estimated_transform = output_dict['estimated_transform']\n",
    "    transform = data_dict['transform']\n",
    "\n",
    "    rre, rte = compute_registration_error(transform, estimated_transform)\n",
    "    rre_list.append(rre)\n",
    "    rte_list.append(rte)\n",
    "    print(f\"RRE(deg): {rre:.3f}, RTE(m): {rte:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RRE(deg): 2.883, Mean RTE(m): 0.916\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean RRE(deg): {np.mean(rre_list):.3f}, Mean RTE(m): {np.mean(rte_list):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median RRE(deg): 1.428, Median RTE(m): 0.258\n"
     ]
    }
   ],
   "source": [
    "print(f\"Median RRE(deg): {np.median(rre_list):.3f}, Median RTE(m): {np.median(rte_list):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
